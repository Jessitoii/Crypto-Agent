{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j44etXE_dSrV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import pandas as pd\n",
        "from sentence_transformers import InputExample\n",
        "\n",
        "# ==========================================\n",
        "# 1. DATASET CLASSES\n",
        "# ==========================================\n",
        "class NexusTripletDataset(Dataset):\n",
        "    \"\"\"\n",
        "    NEXUS v4.2: Original_ID bazlƒ± Causal Triplet Mining.\n",
        "    Modelin 'Aynƒ± varlƒ±k, aynƒ± olay, zƒ±t sonu√ß' arasƒ±ndaki farkƒ± \n",
        "    (semantic difference) √∂ƒürenmesini saƒülar.\n",
        "    \"\"\"\n",
        "    def __init__(self, df):\n",
        "        self.samples = []\n",
        "        self.df = df\n",
        "        \n",
        "        # 1. Veriyi original_id bazƒ±nda grupla\n",
        "        # Bu, her haberin varyasyonlarƒ±nƒ± ve kar≈üƒ±tlarƒ±nƒ± bir arada tutar.\n",
        "        self.grouped = df.groupby('original_id')\n",
        "        \n",
        "        # 2. Triplet olu≈üturma d√∂ng√ºs√º\n",
        "        for oid, group in self.grouped:\n",
        "            # Anchor adaylarƒ± (Orijinal veya augmented fark etmez)\n",
        "            long_samples = group[group['label'] == 2]\n",
        "            short_samples = group[group['label'] == 1]\n",
        "            \n",
        "            # --- LONG Anchor Senaryosu ---\n",
        "            for _, anchor_row in long_samples.iterrows():\n",
        "                # Positive: Aynƒ± original_id i√ßindeki diƒüer LONG haber (Augmented versiyon)\n",
        "                # Eƒüer ba≈üka LONG yoksa, global havuzdan aynƒ± asset'e sahip ba≈üka bir LONG √ßek\n",
        "                pos_candidates = long_samples[long_samples.index != anchor_row.name]\n",
        "                \n",
        "                if not pos_candidates.empty:\n",
        "                    positive = pos_candidates.sample(1).iloc[0]['text']\n",
        "                else:\n",
        "                    # Alternatif: Aynƒ± asset'e sahip rastgele bir LONG haber (Dataset geneli)\n",
        "                    asset = anchor_row['text'].split('[C]')[1] if '[C]' in anchor_row['text'] else \"\"\n",
        "                    alt_pos = df[(df['label'] == 2) & (df['text'].str.contains(asset)) & (df.index != anchor_row.name)]\n",
        "                    if not alt_pos.empty:\n",
        "                        positive = alt_pos.sample(1).iloc[0]['text']\n",
        "                    else: continue\n",
        "\n",
        "                # HARD NEGATIVE: ƒ∞≈üte zurnanƒ±n zƒ±rt dediƒüi yer.\n",
        "                # Aynƒ± original_id i√ßindeki SHORT haber (Counterfactual)\n",
        "                # Model 'Bu haber neden LONG deƒüil de SHORT?' sorusuna burada yanƒ±t bulur.\n",
        "                if not short_samples.empty:\n",
        "                    negative = short_samples.sample(1).iloc[0]['text']\n",
        "                    self.samples.append((anchor_row['text'], positive, negative))\n",
        "                \n",
        "            # --- SHORT Anchor Senaryosu ---\n",
        "            for _, anchor_row in short_samples.iterrows():\n",
        "                pos_candidates = short_samples[short_samples.index != anchor_row.name]\n",
        "                \n",
        "                if not pos_candidates.empty:\n",
        "                    positive = pos_candidates.sample(1).iloc[0]['text']\n",
        "                else:\n",
        "                    asset = anchor_row['text'].split('[C]')[1] if '[C]' in anchor_row['text'] else \"\"\n",
        "                    alt_pos = df[(df['label'] == 1) & (df['text'].str.contains(asset)) & (df.index != anchor_row.name)]\n",
        "                    if not alt_pos.empty:\n",
        "                        positive = alt_pos.sample(1).iloc[0]['text']\n",
        "                    else: continue\n",
        "\n",
        "                # HARD NEGATIVE: Aynƒ± original_id i√ßindeki LONG haber (Counterfactual)\n",
        "                if not long_samples.empty:\n",
        "                    negative = long_samples.sample(1).iloc[0]['text']\n",
        "                    self.samples.append((anchor_row['text'], positive, negative))\n",
        "\n",
        "        print(f\"‚úì Created {len(self.samples)} Causal Triplets from {len(self.grouped)} unique news events.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor, pos, neg = self.samples[idx]\n",
        "        return InputExample(texts=[anchor, pos, neg])\n",
        "\n",
        "# ‚úÖ DOƒûRU (yeni)\n",
        "class NexusDatasetV2(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df['text'].tolist()\n",
        "        self.gates = (df['label'] != 0).astype(float).values  # S EKLE\n",
        "        self.directions = np.where(df['label'] == 0, -1, (df['label'] == 2).astype(float))  # S EKLE\n",
        "        self.tps = df['tp_pct'].fillna(0.0).values  # S EKLE\n",
        "        self.validities = df['validity_minutes'].fillna(0.0).values  # S EKLE\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.texts[idx],\n",
        "            torch.tensor(self.gates[idx], dtype=torch.float32),\n",
        "            torch.tensor(self.directions[idx],  dtype=torch.float32),\n",
        "            torch.tensor(self.tps[idx],   dtype=torch.float32),\n",
        "            torch.tensor(self.validities[idx],  dtype=torch.float32),\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwxJQSSmddzW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def calibrate_model(model, val_loader, device):\n",
        "    \"\"\"\n",
        "    Validation set √ºzerinde optimal 'Temperature' deƒüerini bulur.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            texts, gate_gt, _, _, _ = batch\n",
        "            inputs = model.tokenizer(list(texts), return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "            \n",
        "            all_logits.append(outputs['gate'].cpu())\n",
        "            all_labels.append(gate_gt.cpu())\n",
        "\n",
        "    logits = torch.cat(all_logits).numpy()\n",
        "    labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    # Log-loss minimize eden T deƒüerini bul\n",
        "    def objective(T):\n",
        "        scaled_probs = 1 / (1 + np.exp(-logits / T))\n",
        "        return log_loss(labels, scaled_probs)\n",
        "\n",
        "    res = minimize(objective, x0=[1.0], bounds=[(0.01, 10.0)], method='L-BFGS-B')\n",
        "    optimal_t = res.x[0]\n",
        "    \n",
        "    print(f\"‚úì Optimal Temperature Found: {optimal_t:.4f}\")\n",
        "    return optimal_t\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or self.drop_prob == 0:\n",
        "            return x\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        random_tensor.floor_()\n",
        "        return x.div(keep_prob) * random_tensor\n",
        "\n",
        "class TokenAwareAttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Token-level attention ile critical information extraction.\n",
        "    Mean pooling'i √∂ld√ºr, selektif okumayƒ± ba≈ülat.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size=768, num_heads=8, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.drop_path = DropPath(drop_prob=0.2)\n",
        "        # Learnable queries: Her task i√ßin model \"neye bakmalƒ±\" √∂ƒüreniyor\n",
        "        self.task_query = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.mha = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Post-attention processing\n",
        "        self.norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        # Final projection\n",
        "        self.output = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, token_embeddings, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_embeddings: (batch, seq_len, hidden_size)\n",
        "            attention_mask: (batch, seq_len) - padding mask\n",
        "        Returns:\n",
        "            output: (batch, 1)\n",
        "            attn_weights: (batch, 1, seq_len) - interpretability i√ßin\n",
        "        \"\"\"\n",
        "        batch_size = token_embeddings.size(0)\n",
        "\n",
        "        # Query'yi batch'e broadcast et\n",
        "        query = self.task_query.expand(batch_size, -1, -1)\n",
        "        # Key-value attention mask olu≈ütur\n",
        "        if attention_mask is not None:\n",
        "            # attention_mask: (batch, seq_len) -> (batch, 1, seq_len)\n",
        "            key_padding_mask = ~attention_mask.bool()\n",
        "        else:\n",
        "            key_padding_mask = None\n",
        "\n",
        "        # Multi-head attention\n",
        "        # query: (batch, 1, hidden) attends to\n",
        "        # key/value: (batch, seq_len, hidden)\n",
        "        attended, attn_weights = self.mha(\n",
        "            query=query,\n",
        "            key=token_embeddings,\n",
        "            value=token_embeddings,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            need_weights=True\n",
        "        )\n",
        "\n",
        "        # Residual + LayerNorm\n",
        "        x = self.norm1(query + self.drop_path(attended))\n",
        "\n",
        "        # FFN\n",
        "        x = self.norm2(x + self.ffn(x))\n",
        "\n",
        "        # Final output\n",
        "        output = self.output(x.squeeze(1))  # (batch, 1)\n",
        "\n",
        "        return output, attn_weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DeepReasoningBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    NEXUS v4.5: Stacked Transformer Decoder Layers.\n",
        "    Sadece pooling deƒüil, adƒ±m adƒ±m muhakeme yapar.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_layers=2, num_heads=8, dropout=0.2):\n",
        "        super().__init__()\n",
        "        # 1. Learnable Task Query (Karar vekt√∂r√º)\n",
        "        self.query = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
        "        \n",
        "        # 2. Transformer Decoder Katmanlarƒ±\n",
        "        # Not: num_layers=2 finansal nedensellik i√ßin 'sweet spot' noktasƒ±dƒ±r.\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_size * 4,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.reasoning_tower = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        \n",
        "    def forward(self, token_embeddings, mask=None):\n",
        "        batch_size = token_embeddings.size(0)\n",
        "        query = self.query.expand(batch_size, -1, -1)\n",
        "        \n",
        "        # Padding mask'i Transformer formatƒ±na (float mask) √ßeviriyoruz\n",
        "        if mask is not None:\n",
        "            # 0 (padding) -> -inf, 1 (valid) -> 0\n",
        "            key_padding_mask = (mask == 0)\n",
        "        else:\n",
        "            key_padding_mask = None\n",
        "            \n",
        "        # Reasoning Tower: Query, tokenlar √ºzerinde birden fazla kez 'd√º≈ü√ºn√ºr'\n",
        "        # tgt: Query, memory: Encoder (DeBERTa) √ßƒ±ktƒ±larƒ±\n",
        "        x = self.reasoning_tower(\n",
        "            tgt=query,\n",
        "            memory=token_embeddings,\n",
        "            memory_key_padding_mask=key_padding_mask\n",
        "        )\n",
        "        \n",
        "        return x.squeeze(1) # (batch, hidden_size)\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class NexusV2Production(nn.Module):\n",
        "    def __init__(self, backbone_name=\"microsoft/deberta-v3-base\"):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(backbone_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(backbone_name)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "\n",
        "        # DERƒ∞N BLOKLAR (num_layers=2)\n",
        "        # Sadece Pooling deƒüil, hiyerar≈üik muhakeme.\n",
        "        self.gate_reasoner = DeepReasoningBlock(hidden_size, num_layers=2)\n",
        "        self.dir_reasoner = DeepReasoningBlock(hidden_size, num_layers=2)\n",
        "\n",
        "        # √áIKI≈û KATMANLARI\n",
        "        self.gate_out = nn.Linear(hidden_size, 1)\n",
        "        self.dir_out = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        # Regresyon artƒ±k Gate'in 'd√º≈ü√ºn√ºlm√º≈ü' context'inden besleniyor.\n",
        "        self.tp_head = nn.Sequential(nn.Linear(hidden_size, 128), nn.GELU(), nn.Linear(128, 1), nn.Tanh())\n",
        "        self.val_head = nn.Sequential(nn.Linear(hidden_size, 128), nn.GELU(), nn.Linear(128, 1), nn.Tanh())\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, return_attentions=False):\n",
        "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        token_embeddings = encoder_outputs.last_hidden_state\n",
        "\n",
        "        # Muhakeme S√ºreci\n",
        "        # Artƒ±k bir liste olarak attention aƒüƒ±rlƒ±klarƒ±nƒ± da alabilirsin.\n",
        "        gate_context = self.gate_reasoner(token_embeddings, attention_mask)\n",
        "        dir_context = self.dir_reasoner(token_embeddings, attention_mask)\n",
        "\n",
        "        # Karar ve Regresyon\n",
        "        gate_logit = self.gate_out(gate_context)\n",
        "        dir_logit = self.dir_out(dir_context)\n",
        "        tp_out = self.tp_head(gate_context) * 5.0 # Max %5 TP\n",
        "        val_out = self.val_head(gate_context) * 20.0 # Max 20dk\n",
        "\n",
        "        return {\n",
        "            'gate': gate_logit,\n",
        "            'direction': dir_logit,\n",
        "            'tp': tp_out,\n",
        "            'validity': val_out\n",
        "        }\n",
        "\n",
        "    def predict(self, texts, threshold=0.5, temperature=1.0):\n",
        "        \"\"\"\n",
        "        NEXUS v4.5 Inference: Derin muhakeme sonu√ßlarƒ±nƒ± parse eder.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        device = next(self.parameters()).device\n",
        "        \n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=256\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.forward(\n",
        "                input_ids=inputs['input_ids'], \n",
        "                attention_mask=inputs['attention_mask']\n",
        "            )\n",
        "\n",
        "            gate_probs = torch.sigmoid(outputs['gate'] / temperature).cpu().numpy()\n",
        "            dir_probs = torch.sigmoid(outputs['direction'] / temperature).cpu().numpy()\n",
        "            tp_preds = outputs['tp'].cpu().numpy()\n",
        "            val_preds = outputs['validity'].cpu().numpy()\n",
        "\n",
        "        results = []\n",
        "        for i in range(len(texts)):\n",
        "            g_prob = gate_probs[i][0]\n",
        "            d_prob = dir_probs[i][0]\n",
        "            \n",
        "            # Aksiyon Kararƒ±\n",
        "            gate_decision = 1 if g_prob >= threshold else 0\n",
        "            \n",
        "            # G√ºven Skoru Normalizasyonu\n",
        "            dir_conf = d_prob if d_prob >= 0.5 else (1 - d_prob)\n",
        "            \n",
        "            res = {\n",
        "                'text': texts[i],\n",
        "                'gate': gate_decision,\n",
        "                'gate_confidence': round(float(g_prob), 4),\n",
        "                'direction': 'LONG' if d_prob >= 0.5 else 'SHORT',\n",
        "                'direction_confidence': round(float(dir_conf), 4),\n",
        "                # Sadece aksiyon varsa deƒüer ata, yoksa 0.\n",
        "                'take_profit_pct': round(float(tp_preds[i][0]), 2) if gate_decision == 1 else 0.0,\n",
        "                'validity_minutes': round(float(val_preds[i][0]), 1) if gate_decision == 1 else 0.0,\n",
        "            }\n",
        "            results.append(res)\n",
        "\n",
        "        return results if len(results) > 1 else results[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "185be10dd2c94b019333ad40eaa6b637",
            "a184c76134a646f1aa7e137a99fd1f1d",
            "7ddc911873ea4becb1e8fde0b10d537a",
            "f40726cc93b84cdaad35af6f72271dbd",
            "43d2e4feba614833a4a605b61b3c14f1",
            "19b64c46557b4518934763b023903798",
            "8c5e7747f6764410b4794738dfc3575b",
            "0ef741bd06784adc89f6426c7203a330",
            "7a89d1d6f96144d2b7ed0ca3d7255fdb",
            "e1853d4aa5a243179fb14ed6ff08e126",
            "1d5ddce4014b49dd826f2bba415df52b"
          ]
        },
        "id": "SjgwjNGkdnHh",
        "outputId": "10c8ce44-8fcf-459f-90ce-4737f5a63445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "==================================================\n",
            "MODEL SELECTION: DeBERTa-v3-SMALL\n",
            "==================================================\n",
            "‚ö†Ô∏è  Medium/Large model kullanmayƒ±n!\n",
            "Gerekli metrikler i√ßin minimum threshold:\n",
            "  - OOD Gate F1 ‚â• 0.70\n",
            "  - HOLD Precision ‚â• 0.85\n",
            "  - Counterfactual Flip ‚â• 80%\n",
            "Bu threshold'lar a≈üƒ±lmadan model b√ºy√ºtme = overfit!\n",
            "==================================================\n",
            "\n",
            "\n",
            "Loading data...\n",
            "Total samples: 11646\n",
            "Label distribution:\n",
            "label\n",
            "0    8308\n",
            "2    1877\n",
            "1    1461\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Applying augmentation limit...\n",
            "\n",
            "üìä TRAIN + VAL Seti Benzersiz Haber Analizi (Unique IDs):\n",
            "   - Toplam Benzersiz: 8084\n",
            "   - HOLD  (0): 7892 unique news\n",
            "   - SHORT (1): 85 unique news\n",
            "   - LONG  (2): 107 unique news\n",
            "\n",
            "üìä OOD TEST Seti Benzersiz Haber Analizi (Unique IDs):\n",
            "   - Toplam Benzersiz: 425\n",
            "   - HOLD  (0): 416 unique news\n",
            "   - SHORT (1): 6 unique news\n",
            "   - LONG  (2): 3 unique news\n",
            "Train: 9283 | Val: 1770 | OOD Test: 593\n",
            "\n",
            "==================================================\n",
            "STAGE 1: BACKBONE TRAINING WITH TRIPLET LOSS\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,622,016 || all params: 185,453,568 || trainable%: 0.8746\n",
            "Triplet strategy: 70% reasoning, 30% self-contrast\n",
            "\n",
            "Training on 2205 triplets for 3 epochs...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "185be10dd2c94b019333ad40eaa6b637",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/content/wandb/offline-run-20260111_095336-5ynndby4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='414' max='414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [414/414 05:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úì Backbone training complete. Saved to 'nexus-lora-backbone-final'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, models\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, mean_absolute_error, mean_squared_error\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "# ==========================================\n",
        "# TRAINING CONFIGURATION\n",
        "# ==========================================\n",
        "class TrainingConfig:\n",
        "    # Stage 1 options\n",
        "    USE_LORA_STAGE1 = True  # False for Option B\n",
        "    TRIPLET_REASONING_RATIO = 1  # 0.7 = hybrid, 1.0 = full reasoning\n",
        "\n",
        "    # Stage 2 options\n",
        "    FREEZE_LORA_STAGE2 = True  # True for Option A\n",
        "\n",
        "    # Model selection\n",
        "    BACKBONE_MODEL = \"microsoft/DeBERTa-v3-base\"  # Don't change to medium/large yet!\n",
        "\n",
        "    # Data\n",
        "    AUGMENTATION_MAX_PER_ORIGINAL = 8\n",
        "    OOD_TEST_RATIO = 0.05\n",
        "\n",
        "    # Training\n",
        "    STAGE1_EPOCHS = 3\n",
        "    STAGE2_EPOCHS = 5\n",
        "    BATCH_SIZE = 16\n",
        "\n",
        "    # Thresholds\n",
        "    OOD_F1_THRESHOLD = 0.70\n",
        "    HOLD_PRECISION_THRESHOLD = 0.85\n",
        "    CAUSALITY_THRESHOLD = 0.80\n",
        "\n",
        "config = TrainingConfig()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. LORA CONFIGURATION - GPT FIXED\n",
        "# ==========================================\n",
        "def apply_lora_final(st_model, r=16):\n",
        "    \"\"\"\n",
        "    GPT Fix: output.dense √áIKARILDI\n",
        "    Sadece attention + intermediate.dense\n",
        "    \"\"\"\n",
        "    target_model = st_model._first_module().auto_model\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=r,\n",
        "        lora_alpha=r*2,\n",
        "        target_modules=[\n",
        "            \"query_proj\",\n",
        "            \"key_proj\",\n",
        "            \"value_proj\",\n",
        "            \"intermediate.dense\"\n",
        "            # output.dense √áIKARILDI - representation collapse riski\n",
        "        ],\n",
        "        lora_dropout=0.3,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.FEATURE_EXTRACTION\n",
        "    )\n",
        "\n",
        "    st_model._first_module().auto_model = get_peft_model(target_model, lora_config)\n",
        "    st_model._first_module().auto_model.print_trainable_parameters()\n",
        "    return st_model\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3. STAGE 1: BACKBONE TRAINING\n",
        "# ==========================================\n",
        "class CausalTripletTrainer:\n",
        "    def __init__(self, model, train_loader, margin=0.3, lr_backbone=1e-5, lr_reasoner=1e-4):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.device = next(model.parameters()).device\n",
        "        \n",
        "        # Triplet Loss: Cosine Distance tabanlƒ±\n",
        "        self.criterion = nn.TripletMarginWithDistanceLoss(\n",
        "            distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y),\n",
        "            margin=margin\n",
        "        )\n",
        "        \n",
        "        # Diferansiyel Learning Rate: Reasoner 'Hot', LoRA 'Warm'\n",
        "        self.optimizer = AdamW([\n",
        "            {'params': [p for n, p in model.encoder.named_parameters() if \"lora_\" in n], 'lr': lr_backbone},\n",
        "            {'params': model.gate_reasoner.parameters(), 'lr': lr_reasoner}\n",
        "        ], weight_decay=0.01)\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(self.train_loader, desc=\"Stage 1: Causal Triplet Training\")\n",
        "        \n",
        "        for batch in pbar:\n",
        "            # Batch: (anchor, positive, negative)\n",
        "            anchors, positives, negatives = batch\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            # 1. Embeddings: Her √º√ß haber de gate_reasoner'dan ge√ßip 'Context Vector' √ºretir.\n",
        "            # Bu vekt√∂r, modelin o haber hakkƒ±ndaki 'rafine d√º≈ü√ºncesidir'.\n",
        "            a_emb = self._get_embedding(anchors)\n",
        "            p_emb = self._get_embedding(positives)\n",
        "            n_emb = self._get_embedding(negatives)\n",
        "            \n",
        "            # 2. Loss Calculation\n",
        "            loss = self.criterion(a_emb, p_emb, n_emb)\n",
        "            \n",
        "            # 3. Optimization\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0) # Gradyan patlamasƒ± korumasƒ±\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({'triplet_loss': f'{loss.item():.4f}'})\n",
        "            \n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    def _get_embedding(self, texts):\n",
        "        \"\"\"Haber metnini alƒ±p gate_reasoner √ßƒ±kƒ±≈üƒ±ndaki 768d context vekt√∂r√ºn√º d√∂ner.\"\"\"\n",
        "        inputs = self.model.tokenizer(\n",
        "            list(texts), return_tensors=\"pt\", padding=True, truncation=True, max_length=256\n",
        "        ).to(self.device)\n",
        "        \n",
        "        # Forward pass: Sadece context vekt√∂r√ºn√º alƒ±yoruz (logitleri deƒüil)\n",
        "        encoder_out = self.model.encoder(inputs['input_ids'], inputs['attention_mask']).last_hidden_state\n",
        "        context, _ = self.model.gate_reasoner(encoder_out, inputs['attention_mask'])\n",
        "        return context\n",
        "\n",
        "def get_nexus_optimizer(model, base_lr=1e-4, lora_lr=1e-5):\n",
        "    \"\"\"\n",
        "    NEXUS Differential Learning Rate Strategy.\n",
        "    1. Backbone (DeBERTa): Frozen (0 LR)\n",
        "    2. LoRA Layers: Warm (1e-5) - Hizalama i√ßin esnek bƒ±rakƒ±ldƒ±.\n",
        "    3. Reasoning Blocks: Hot (1e-4) - Yeni mantƒ±ƒüƒ± bunlar √∂ƒürenecek.\n",
        "    4. Task Heads: Hot (1e-4)\n",
        "    \"\"\"\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    \n",
        "    optimizer_grouped_parameters = [\n",
        "        # Grup 1: Yeni eklenen Reasoning Blocklar ve Headler (Y√ºksek LR)\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() \n",
        "                       if (\"reasoner\" in n or \"out\" in n or \"head\" in n) \n",
        "                       and not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.05,\n",
        "            \"lr\": base_lr,\n",
        "        },\n",
        "        # Grup 2: LoRA Adapt√∂rleri (D√º≈ü√ºk LR - Backbone'u bozmadan esnet)\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() \n",
        "                       if \"lora_\" in n and not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "            \"lr\": lora_lr,\n",
        "        },\n",
        "        # Bias ve LayerNormlar i√ßin Weight Decay kapatma\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() \n",
        "                       if any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "            \"lr\": base_lr,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, eps=1e-8)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def setup_scheduler(optimizer, train_loader, epochs=8):\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    # Warmup oranƒ±nƒ± %15'e √ßƒ±karƒ±yoruz √ß√ºnk√º yeni bloklarƒ±mƒ±z √ßok 'ham'\n",
        "    warmup_steps = int(total_steps * 0.15)\n",
        "    \n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "    return scheduler\n",
        "\n",
        "# ==========================================\n",
        "# 4. COMBINED LOSS - GPT FIXED\n",
        "# ==========================================\n",
        "def nexus_combined_loss(outputs, targets, gamma=2.0, label_smoothing=0.1):\n",
        "    \"\"\"\n",
        "    NEXUS v7_Final: DeepReasoningBlock ile tam uyumlu.\n",
        "    Verbalizer y√ºk√º atƒ±ldƒ±, t√ºm baskƒ± 'Directional Causality' √ºzerine kuruldu.\n",
        "    \"\"\"\n",
        "    gate_p = outputs['gate']\n",
        "    dir_p = outputs['direction']\n",
        "    tp_p = outputs['tp']\n",
        "    val_p = outputs['validity']\n",
        "\n",
        "    gate_gt, dir_gt, tp_gt, val_gt = targets\n",
        "\n",
        "    # 1. GATE LOSS: Kalibre edilmi≈ü BCE\n",
        "    # Aksiyon sinyallerini (2.0 weight) 'Hold' g√ºr√ºlt√ºs√ºnden ayƒ±rƒ±r.\n",
        "    gate_gt_smoothed = gate_gt * (1 - label_smoothing) + 0.5 * label_smoothing\n",
        "    loss_gate = F.binary_cross_entropy_with_logits(\n",
        "        gate_p, gate_gt_smoothed, pos_weight=torch.tensor([2.0]).to(gate_p.device)\n",
        "    )\n",
        "\n",
        "    # 2. DIRECTIONAL FOCAL LOSS (En Kritik B√∂lge)\n",
        "    # Verbalizer yok, bu y√ºzden modelin tek rehberi burasƒ±.\n",
        "    valid_dir_mask = (dir_gt >= 0).squeeze()\n",
        "    if valid_dir_mask.sum() > 0:\n",
        "        # Bullish Bias Killer: SHORT (0) hala 3 kat daha maliyetli.\n",
        "        dir_weights = torch.where(dir_gt[valid_dir_mask] == 0, 3.0, 1.0)\n",
        "        \n",
        "        probs = torch.sigmoid(dir_p[valid_dir_mask])\n",
        "        pt = torch.where(dir_gt[valid_dir_mask] == 1, probs, 1 - probs)\n",
        "        focal_weight = (1 - pt) ** gamma # Zor √∂rnekleri (zƒ±t haberler) daha sert cezalandƒ±r.\n",
        "        \n",
        "        bce_dir = F.binary_cross_entropy_with_logits(\n",
        "            dir_p[valid_dir_mask], \n",
        "            dir_gt[valid_dir_mask], \n",
        "            reduction='none'\n",
        "        )\n",
        "        loss_dir = (dir_weights * focal_weight * bce_dir).mean()\n",
        "    else:\n",
        "        loss_dir = torch.tensor(0.0, device=gate_p.device)\n",
        "\n",
        "    # 3. REGRESSION LOSS: Robust Huber\n",
        "    # TP ve Validity artƒ±k 'D√º≈ü√ºn√ºlm√º≈ü Context'ten geliyor.\n",
        "    action_mask = (gate_gt == 1).squeeze()\n",
        "    if action_mask.sum() > 0:\n",
        "        loss_tp = F.huber_loss(tp_p[action_mask], tp_gt[action_mask], delta=1.0)\n",
        "        loss_val = F.huber_loss(val_p[action_mask], val_gt[action_mask], delta=1.0)\n",
        "    else:\n",
        "        loss_tp = loss_val = torch.tensor(0.0, device=gate_p.device)\n",
        "\n",
        "    # TOPLAM KAYIP: Y√∂n doƒüruluƒüu (Reasoning) ana √∂ncelik.\n",
        "    # Dir (4.0) > Gate (1.0) > Regression (0.5)\n",
        "    total_loss = (1.0 * loss_gate) + (4.0 * loss_dir) + (0.5 * (loss_tp + loss_val))\n",
        "\n",
        "    return total_loss, {\n",
        "        'gate': loss_gate.item(),\n",
        "        'dir_focal': loss_dir.item(),\n",
        "        'tp_huber': loss_tp.item(),\n",
        "        'val_huber': loss_val.item()\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# 6. OUT-OF-DISTRIBUTION TEST\n",
        "# ==========================================\n",
        "def create_ood_test_set(df, ood_ratio=0.05):\n",
        "    \"\"\"Her class'tan %5'ini √ßek\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CREATING OUT-OF-DISTRIBUTION TEST SET\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    long_df = df[df['label'] == 2]\n",
        "    short_df = df[df['label'] == 1]\n",
        "    hold_df = df[df['label'] == 0]\n",
        "\n",
        "    ood_long = long_df.sample(frac=ood_ratio, random_state=42)\n",
        "    ood_short = short_df.sample(frac=ood_ratio, random_state=42)\n",
        "    ood_hold = hold_df.sample(frac=ood_ratio, random_state=42)\n",
        "\n",
        "    ood_test = pd.concat([ood_long, ood_short, ood_hold]).reset_index(drop=True)\n",
        "    train_df = df[~df.index.isin(ood_test.index)].reset_index(drop=True)\n",
        "\n",
        "    print(f\"OOD Test Set: {len(ood_test)} samples\")\n",
        "    print(f\"  - Long: {len(ood_long)} | Short: {len(ood_short)} | Hold: {len(ood_hold)}\")\n",
        "    print(f\"Remaining Train: {len(train_df)} samples\")\n",
        "\n",
        "    return train_df, ood_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 7. COUNTERFACTUAL TEST\n",
        "# ==========================================\n",
        "def counterfactual_test(model):\n",
        "    \"\"\"Causal reasoning testi\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"COUNTERFACTUAL CAUSALITY TEST\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    test_pairs = [\n",
        "        # Basic flip (mevcut)\n",
        "        (\"[N] Bitcoin ETF approved by SEC [C] BTC\",\n",
        "        \"[N] Bitcoin ETF rejected by SEC [C] BTC\"),\n",
        "        (\"[N] Ethereum upgrade successful [C] ETH\",\n",
        "        \"[N] Ethereum upgrade delayed [C] ETH\"),\n",
        "\n",
        "        # Numeric reversal\n",
        "        (\"[N] Solana gains 15% in morning trading session [C] SOL\",\n",
        "        \"[N] Solana drops 15% in morning trading session [C] SOL\"),\n",
        "\n",
        "        # Subtle negation\n",
        "        (\"[N] Concerns over Polygon network stability prove unfounded [C] MATIC\",\n",
        "        \"[N] Concerns over Polygon network stability prove justified [C] MATIC\"),\n",
        "\n",
        "        # Implicit sentiment\n",
        "        (\"[N] Bitcoin finally breaks resistance at $70K after months [C] BTC\",\n",
        "        \"[N] Bitcoin fails again to break resistance at $70K [C] BTC\"),\n",
        "\n",
        "        # Multi-entity (tricky)\n",
        "        (\"[N] Ethereum outperforms Bitcoin in Q4 rally [C] ETH\",\n",
        "        \"[N] Bitcoin outperforms Ethereum in Q4 rally [C] BTC\"),\n",
        "\n",
        "        # Sarcastic/Ironic (advanced)\n",
        "        (\"[N] Experts say crypto winter is 'definitely' over this time [C] BTC\",\n",
        "        \"[N] Experts confirm crypto winter continues with no end in sight [C] BTC\"),\n",
        "    ]\n",
        "\n",
        "    passed = 0\n",
        "    total = len(test_pairs)\n",
        "\n",
        "    for orig, counter in test_pairs:\n",
        "        orig_pred = model.predict(orig, temperature=optimal_t)\n",
        "        counter_pred = model.predict(counter, temperature=optimal_t)\n",
        "\n",
        "        print(f\"\\n{'‚îÄ'*50}\")\n",
        "        print(f\"Original: {orig}\")\n",
        "        print(f\"  ‚Üí Gate: {orig_pred['gate']} | Dir: {orig_pred['direction']} | TP: {orig_pred['take_profit_pct']:.2f}%\")\n",
        "        print(f\"Counterfactual: {counter}\")\n",
        "        print(f\"  ‚Üí Gate: {counter_pred['gate']} | Dir: {counter_pred['direction']} | TP: {counter_pred['take_profit_pct']:.2f}%\")\n",
        "\n",
        "        if orig_pred['direction'] != counter_pred['direction']:\n",
        "            print(\"  ‚úì PASS: Direction flipped correctly\")\n",
        "            passed += 1\n",
        "        else:\n",
        "            print(\"  ‚úó FAIL: Direction didn't flip\")\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Counterfactual Test Result: {passed}/{total} passed ({passed/total*100:.1f}%)\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    return passed / total\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 8. MAIN TRAINING PIPELINE - GPT FIXED\n",
        "# ==========================================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "# YENƒ∞ EKLEME - Kullanƒ±cƒ± uyarƒ±sƒ±\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL SELECTION: DeBERTa-v3-SMALL\")\n",
        "print(\"=\"*50)\n",
        "print(\"‚ö†Ô∏è  Medium/Large model kullanmayƒ±n!\")\n",
        "print(\"Gerekli metrikler i√ßin minimum threshold:\")\n",
        "print(\"  - OOD Gate F1 ‚â• 0.70\")\n",
        "print(\"  - HOLD Precision ‚â• 0.85\")\n",
        "print(\"  - Counterfactual Flip ‚â• 80%\")\n",
        "print(\"Bu threshold'lar a≈üƒ±lmadan model b√ºy√ºtme = overfit!\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "# Load data\n",
        "print(\"\\nLoading data...\")\n",
        "df = pd.read_json('/content/nexus_elite_dataset.json')\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
        "# Augmentation limit\n",
        "print(\"\\nApplying augmentation limit...\")\n",
        "\n",
        "    # OOD split\n",
        "def create_true_ood_split_with_report(df, ood_ratio=0.05):\n",
        "    \"\"\"ID bazlƒ± split yapar ve benzersiz haber daƒüƒ±lƒ±mƒ±nƒ± raporlar.\"\"\"\n",
        "    unique_ids = df['original_id'].unique()\n",
        "    np.random.shuffle(unique_ids)\n",
        "\n",
        "    split_idx = int(len(unique_ids) * ood_ratio)\n",
        "    ood_ids = unique_ids[:split_idx]\n",
        "\n",
        "    train_df = df[~df['original_id'].isin(ood_ids)].reset_index(drop=True)\n",
        "    ood_df = df[df['original_id'].isin(ood_ids)].reset_index(drop=True)\n",
        "\n",
        "    def get_id_stats(target_df, name):\n",
        "        # Her label i√ßin benzersiz ID'leri say\n",
        "        stats = target_df.groupby('label')['original_id'].nunique()\n",
        "        total_unique = target_df['original_id'].nunique()\n",
        "        print(f\"\\nüìä {name} Seti Benzersiz Haber Analizi (Unique IDs):\")\n",
        "        print(f\"   - Toplam Benzersiz: {total_unique}\")\n",
        "        print(f\"   - HOLD  (0): {stats.get(0, 0)} unique news\")\n",
        "        print(f\"   - SHORT (1): {stats.get(1, 0)} unique news\")\n",
        "        print(f\"   - LONG  (2): {stats.get(2, 0)} unique news\")\n",
        "        return stats\n",
        "\n",
        "    # Raporlarƒ± yazdƒ±r\n",
        "    get_id_stats(train_df, \"TRAIN + VAL\")\n",
        "    get_id_stats(ood_df, \"OOD TEST\")\n",
        "\n",
        "    return train_df, ood_df\n",
        "\n",
        "# Ana akƒ±≈üta bunu kullan:\n",
        "train_df, ood_test_df = create_true_ood_split_with_report(df, ood_ratio=0.05)\n",
        "\n",
        "# Train/Val split\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(train_df, groups=train_df['original_id']))\n",
        "train_df_final = train_df.iloc[train_idx].reset_index(drop=True)\n",
        "val_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "print(f\"Train: {len(train_df_final)} | Val: {len(val_df)} | OOD Test: {len(ood_test_df)}\")\n",
        "\n",
        "# STAGE 1: Triplet Training\n",
        "trainer = CausalTripletTrainer(model, triplet_loader, margin=0.3)\n",
        "for epoch in range(config.STAGE1_EPOCHS):\n",
        "    avg_loss = trainer.train_epoch()\n",
        "    print(f\"Epoch {epoch} Triplet Loss: {avg_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqlQAA05d5Cp",
        "outputId": "7fe50f48-6773-4626-bcdc-e7c34d16ba7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "==================================================\n",
            "MODEL SELECTION: DeBERTa-v3-SMALL\n",
            "==================================================\n",
            "‚ö†Ô∏è  Medium/Large model kullanmayƒ±n!\n",
            "Gerekli metrikler i√ßin minimum threshold:\n",
            "  - OOD Gate F1 ‚â• 0.70\n",
            "  - HOLD Precision ‚â• 0.85\n",
            "  - Counterfactual Flip ‚â• 80%\n",
            "Bu threshold'lar a≈üƒ±lmadan model b√ºy√ºtme = overfit!\n",
            "==================================================\n",
            "\n",
            "\n",
            "Loading data...\n",
            "Total samples: 11646\n",
            "Label distribution:\n",
            "label\n",
            "0    8308\n",
            "2    1877\n",
            "1    1461\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Applying augmentation limit...\n",
            "\n",
            "üìä TRAIN + VAL Seti Benzersiz Haber Analizi (Unique IDs):\n",
            "   - Toplam Benzersiz: 8084\n",
            "   - HOLD  (0): 7897 unique news\n",
            "   - SHORT (1): 84 unique news\n",
            "   - LONG  (2): 103 unique news\n",
            "\n",
            "üìä OOD TEST Seti Benzersiz Haber Analizi (Unique IDs):\n",
            "   - Toplam Benzersiz: 425\n",
            "   - HOLD  (0): 411 unique news\n",
            "   - SHORT (1): 7 unique news\n",
            "   - LONG  (2): 7 unique news\n",
            "Train: 9240 | Val: 1770 | OOD Test: 636\n",
            "\n",
            "‚ö†Ô∏è  OPTION A: Freezing LoRA in Stage 2\n",
            "\n",
            "==================================================\n",
            "STAGE 2: MULTI-HEAD TRAINING\n",
            "==================================================\n",
            "\n",
            "‚ö†Ô∏è  Using WeightedRandomSampler (pos_weight=1.0)\n",
            "Class balance: Hold=6720, Action=2520\n",
            "Sampler weights: Hold=0.012, Action=0.020\n",
            "\n",
            "‚ö†Ô∏è  OPTION A: Freezing LoRA weights in Stage 2\n",
            "‚úì All encoder weights frozen. Only task heads will be trained.\n",
            "Optimizer: Only training task heads (encoder frozen)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/12 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 578/578 [03:34<00:00,  2.70it/s, loss=7.1555]\n",
            "Epoch 1/5 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [00:34<00:00,  3.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Epoch 1/5 Results\n",
            "==================================================\n",
            "Train Loss: 27.5011 | Val Loss: 17.7543\n",
            "Gate - Acc: 0.985 | Prec: 0.995 | Rec: 0.960 | F1: 0.977\n",
            "Direction Acc: 0.715\n",
            "TP MAE: 2.27\n",
            "Validity MAE: 4.27\n",
            "‚úì Model saved (val_loss: 17.7543)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/12 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 578/578 [03:29<00:00,  2.76it/s, loss=1.9068]\n",
            "Epoch 2/5 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [00:35<00:00,  3.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Epoch 2/5 Results\n",
            "==================================================\n",
            "Train Loss: 18.3536 | Val Loss: 17.6702\n",
            "Gate - Acc: 0.989 | Prec: 0.997 | Rec: 0.971 | F1: 0.984\n",
            "Direction Acc: 0.740\n",
            "TP MAE: 2.2\n",
            "Validity MAE: 4.29\n",
            "‚úì Model saved (val_loss: 17.6702)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/12 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 578/578 [03:30<00:00,  2.75it/s, loss=21.8279]\n",
            "Epoch 3/5 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [00:35<00:00,  3.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Epoch 3/5 Results\n",
            "==================================================\n",
            "Train Loss: 17.6318 | Val Loss: 17.5802\n",
            "Gate - Acc: 0.989 | Prec: 0.995 | Rec: 0.971 | F1: 0.983\n",
            "Direction Acc: 0.769\n",
            "TP MAE: 2.21\n",
            "Validity MAE: 4.27\n",
            "‚úì Model saved (val_loss: 17.5802)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/12 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 578/578 [03:23<00:00,  2.85it/s, loss=13.7985]\n",
            "Epoch 4/5 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [00:34<00:00,  3.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Epoch 4/5 Results\n",
            "==================================================\n",
            "Train Loss: 17.5005 | Val Loss: 17.8626\n",
            "Gate - Acc: 0.991 | Prec: 0.995 | Rec: 0.978 | F1: 0.986\n",
            "Direction Acc: 0.776\n",
            "TP MAE: 2.24\n",
            "Validity MAE: 4.13\n",
            "‚úó No improvement (1/3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/12 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 578/578 [03:31<00:00,  2.73it/s, loss=46.4645]\n",
            "Epoch 5/5 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [00:35<00:00,  3.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Epoch 5/5 Results\n",
            "==================================================\n",
            "Train Loss: 17.3882 | Val Loss: 17.6634\n",
            "Gate - Acc: 0.991 | Prec: 0.995 | Rec: 0.978 | F1: 0.986\n",
            "Direction Acc: 0.767\n",
            "TP MAE: 2.21\n",
            "Validity MAE: 4.23\n",
            "‚úó No improvement (2/3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/12 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 578/578 [03:26<00:00,  2.80it/s, loss=14.4418]\n",
            "Epoch 6/5 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [00:34<00:00,  3.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Epoch 6/5 Results\n",
            "==================================================\n",
            "Train Loss: 17.2890 | Val Loss: 17.6634\n",
            "Gate - Acc: 0.991 | Prec: 0.995 | Rec: 0.978 | F1: 0.986\n",
            "Direction Acc: 0.767\n",
            "TP MAE: 2.21\n",
            "Validity MAE: 4.23\n",
            "‚úó No improvement (3/3)\n",
            "Early stopping triggered!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, models\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, mean_absolute_error, mean_squared_error\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def create_true_ood_split_with_report(df, ood_ratio=0.05):\n",
        "    \"\"\"ID bazlƒ± split yapar ve benzersiz haber daƒüƒ±lƒ±mƒ±nƒ± raporlar.\"\"\"\n",
        "    unique_ids = df['original_id'].unique()\n",
        "    np.random.shuffle(unique_ids)\n",
        "\n",
        "    split_idx = int(len(unique_ids) * ood_ratio)\n",
        "    ood_ids = unique_ids[:split_idx]\n",
        "\n",
        "    train_df = df[~df['original_id'].isin(ood_ids)].reset_index(drop=True)\n",
        "    ood_df = df[df['original_id'].isin(ood_ids)].reset_index(drop=True)\n",
        "    def get_id_stats(target_df, name):\n",
        "        # Her label i√ßin benzersiz ID'leri say\n",
        "        stats = target_df.groupby('label')['original_id'].nunique()\n",
        "        total_unique = target_df['original_id'].nunique()\n",
        "        print(f\"\\nüìä {name} Seti Benzersiz Haber Analizi (Unique IDs):\")\n",
        "        print(f\"   - Toplam Benzersiz: {total_unique}\")\n",
        "        print(f\"   - HOLD  (0): {stats.get(0, 0)} unique news\")\n",
        "        print(f\"   - SHORT (1): {stats.get(1, 0)} unique news\")\n",
        "        print(f\"   - LONG  (2): {stats.get(2, 0)} unique news\")\n",
        "        return stats\n",
        "    # Raporlarƒ± yazdƒ±r\n",
        "    get_id_stats(train_df, \"TRAIN + VAL\")\n",
        "    get_id_stats(ood_df, \"OOD TEST\")\n",
        "\n",
        "    return train_df, ood_df\n",
        "\n",
        "# Ana akƒ±≈üta bunu kullan:\n",
        "train_df, ood_test_df = create_true_ood_split_with_report(df, ood_ratio=0.05)\n",
        "# Train/Val split\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(train_df, groups=train_df['original_id']))\n",
        "train_df_final = train_df.iloc[train_idx].reset_index(drop=True)\n",
        "val_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
        "print(f\"Train: {len(train_df_final)} | Val: {len(val_df)} | OOD Test: {len(ood_test_df)}\")\n",
        "\n",
        "model = NexusV2Production(\"nexus-lora-backbone-final\").to(device)  # ‚Üê Stage 1'den y√ºkle\n",
        "optimizer = get_nexus_optimizer(model)\n",
        "scheduler = setup_scheduler(optimizer, train_loader)\n",
        "# Stage 2 LoRA freeze\n",
        "if config.FREEZE_LORA_STAGE2:\n",
        "    print(\"\\n‚ö†Ô∏è  OPTION A: Freezing LoRA in Stage 2\")\n",
        "    for name, param in model.encoder.named_parameters():\n",
        "        param.requires_grad = False\n",
        "else:\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"lora_\" in name:\n",
        "            param.requires_grad = True\n",
        "        elif \"classifier\" in name or \"regressor\" in name:\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "# Stage 2: Multi-head training\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STAGE 2: MULTI-HEAD TRAINING\")\n",
        "print(\"=\"*50)\n",
        "# GPT Fix: POS_WEIGHT KALDIRILDI\n",
        "print(\"\\n‚ö†Ô∏è  Using WeightedRandomSampler (pos_weight=1.0)\")\n",
        "gate_pos_weight = torch.tensor([4.0], dtype=torch.float32, device=device)  # Kullanƒ±lmayacak\n",
        "# Weighted sampler\n",
        "train_labels = (train_df_final['label'] != 0).astype(int).values\n",
        "class_counts = np.bincount(train_labels)\n",
        "# ‚úÖ DOƒûRU (yeni)\n",
        "class_weights = np.sqrt(1.0 / class_counts)\n",
        "sample_weights = class_weights[train_labels]\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True,\n",
        ")\n",
        "print(f\"Class balance: Hold={class_counts[0]}, Action={class_counts[1]}\")\n",
        "print(f\"Sampler weights: Hold={class_weights[0]:.3f}, Action={class_weights[1]:.3f}\")\n",
        "# Datasets\n",
        "train_dataset = NexusDatasetV2(train_df_final)\n",
        "val_dataset = NexusDatasetV2(val_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "# Model\n",
        "# Freeze non-LoRA\n",
        "for name, param in model.encoder.named_parameters():\n",
        "    if \"lora\" not in name:\n",
        "        param.requires_grad = False\n",
        "# ‚úÖ EKLE - LoRA'yƒ± da freeze et (Option A)\n",
        "print(\"\\n‚ö†Ô∏è  OPTION A: Freezing LoRA weights in Stage 2\")\n",
        "for name, param in model.encoder.named_parameters():\n",
        "    # T√ºm encoder'ƒ± freeze et (LoRA dahil)\n",
        "    param.requires_grad = False\n",
        "print(\"‚úì All encoder weights frozen. Only task heads will be trained.\")\n",
        "# Scheduler\n",
        "num_training_steps = len(train_loader) * 5\n",
        "# GPT Fix: Early stopping SADECE val_loss\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "max_epochs = 12\n",
        "optimal_t = calibrate_model(model, val_loader, device)\n",
        "for epoch in range(max_epochs):\n",
        "    # TRAIN\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{max_epochs} [TRAIN]\")\n",
        "    for texts, gate_gt, dir_gt, tp_gt, val_gt in pbar:\n",
        "        optimizer.zero_grad()\n",
        "        gate_gt = gate_gt.view(-1, 1).to(device)\n",
        "        dir_gt = dir_gt.view(-1, 1).to(device)\n",
        "        tp_gt = tp_gt.view(-1, 1).to(device)\n",
        "        val_gt = val_gt.view(-1, 1).to(device)\n",
        "        inputs = model.tokenizer(list(texts), return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
        "        preds = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], gate_labels=gate_gt)\n",
        "        loss, _ = nexus_combined_loss(preds, (gate_gt, dir_gt, tp_gt, val_gt))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "        \n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    # VALIDATION\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_gate_preds = []\n",
        "    all_gate_gts = []\n",
        "    all_dir_preds = []\n",
        "    all_dir_gts = []\n",
        "    all_tp_preds = []\n",
        "    all_tp_gts = []\n",
        "    all_val_preds = []\n",
        "    all_val_gts = []\n",
        "    with torch.no_grad():\n",
        "        for texts, gate_gt, dir_gt, tp_gt, val_gt in tqdm(val_loader, desc=f\"Epoch {epoch+1}/5 [VAL]\"):\n",
        "            gate_gt = gate_gt.view(-1, 1).to(device)\n",
        "            dir_gt = dir_gt.view(-1, 1).to(device)\n",
        "            tp_gt = tp_gt.view(-1, 1).to(device)\n",
        "            val_gt = val_gt.view(-1, 1).to(device)\n",
        "            inputs = model.tokenizer(list(texts), return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
        "            preds = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], gate_labels=gate_gt)\n",
        "            loss, _ = nexus_combined_loss(preds, (gate_gt, dir_gt, tp_gt, val_gt))\n",
        "            val_loss += loss.item()\n",
        "            gate_p = preds['gate']\n",
        "            dir_p = preds['direction']\n",
        "            tp_p = preds['tp']\n",
        "            val_p = preds['validity']\n",
        "            gate_pred = (torch.sigmoid(gate_p) > 0.5).float()\n",
        "            all_gate_preds.extend(gate_pred.cpu().numpy())\n",
        "            all_gate_gts.extend(gate_gt.cpu().numpy())\n",
        "            action_mask = (gate_gt == 1).squeeze()\n",
        "            valid_dir_mask = (dir_gt >= 0).squeeze()  # -1 olan HOLD'larƒ± filtrele\n",
        "            combined_mask = action_mask & valid_dir_mask\n",
        "            if combined_mask.sum() > 0:\n",
        "                dir_pred = (torch.sigmoid(dir_p[combined_mask]) > 0.5).float()\n",
        "                all_dir_preds.extend(dir_pred.cpu().numpy())\n",
        "                all_dir_gts.extend(dir_gt[combined_mask].cpu().numpy())\n",
        "                all_tp_preds.extend(tp_p[action_mask].cpu().numpy())\n",
        "                all_tp_gts.extend(tp_gt[action_mask].cpu().numpy())\n",
        "                all_val_preds.extend(val_p[action_mask].cpu().numpy())\n",
        "                all_val_gts.extend(val_gt[action_mask].cpu().numpy())\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    # Metrics\n",
        "    gate_acc = accuracy_score(all_gate_gts, all_gate_preds)\n",
        "    gate_prec, gate_rec, gate_f1, _ = precision_recall_fscore_support(\n",
        "        all_gate_gts, all_gate_preds, average='binary', zero_division=0\n",
        "    )\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Epoch {epoch+1}/5 Results\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Gate - Acc: {gate_acc:.3f} | Prec: {gate_prec:.3f} | Rec: {gate_rec:.3f} | F1: {gate_f1:.3f}\")\n",
        "    if len(all_dir_preds) > 0:\n",
        "        dir_acc = accuracy_score(all_dir_gts, all_dir_preds)\n",
        "        tp_mae = mean_absolute_error(all_tp_gts, all_tp_preds)\n",
        "        val_mae = mean_absolute_error(all_val_gts, all_val_preds)\n",
        "        print(f\"Direction Acc: {dir_acc:.3f}\")\n",
        "        print(f\"TP MAE: {tp_mae:.3}\")\n",
        "        print(f\"Validity MAE: {val_mae:.3}\")\n",
        "# Early stopping - SADECE val_loss\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), \"nexus_best.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': avg_val_loss,\n",
        "            'val_f1': gate_f1\n",
        "        }, \"nexus_checkpoint.pt\")\n",
        "        print(f\"‚úì Model saved (val_loss: {avg_val_loss:.4f})\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"‚úó No improvement ({patience_counter}/3)\")\n",
        "        if patience_counter >= 3:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h1J8LflwoR5w",
        "outputId": "b8ea9cfc-e336-42a6-f38c-5fd7ddd8d3b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "LOADING BEST MODEL\n",
            "==================================================\n",
            "‚úì Best model loaded\n",
            "\n",
            "==================================================\n",
            "COUNTERFACTUAL CAUSALITY TEST\n",
            "==================================================\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Original: [N] Bitcoin ETF approved by SEC [C] BTC\n",
            "  ‚Üí Gate: 0 | Dir: LONG | TP: 0.99%\n",
            "Counterfactual: [N] Bitcoin ETF rejected by SEC [C] BTC\n",
            "  ‚Üí Gate: 0 | Dir: SHORT | TP: 0.89%\n",
            "  ‚úì PASS: Direction flipped correctly\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Original: [N] Ethereum upgrade successful [C] ETH\n",
            "  ‚Üí Gate: 0 | Dir: LONG | TP: 1.21%\n",
            "Counterfactual: [N] Ethereum upgrade delayed [C] ETH\n",
            "  ‚Üí Gate: 0 | Dir: SHORT | TP: 1.21%\n",
            "  ‚úì PASS: Direction flipped correctly\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Original: [N] Solana gains 15% in morning trading session [C] SOL\n",
            "  ‚Üí Gate: 1 | Dir: LONG | TP: 0.95%\n",
            "Counterfactual: [N] Solana drops 15% in morning trading session [C] SOL\n",
            "  ‚Üí Gate: 1 | Dir: LONG | TP: 0.87%\n",
            "  ‚úó FAIL: Direction didn't flip\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Original: [N] Concerns over Polygon network stability prove unfounded [C] MATIC\n",
            "  ‚Üí Gate: 0 | Dir: LONG | TP: 0.95%\n",
            "Counterfactual: [N] Concerns over Polygon network stability prove justified [C] MATIC\n",
            "  ‚Üí Gate: 0 | Dir: LONG | TP: 0.96%\n",
            "  ‚úó FAIL: Direction didn't flip\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Original: [N] Bitcoin finally breaks resistance at $70K after months [C] BTC\n",
            "  ‚Üí Gate: 0 | Dir: LONG | TP: 0.66%\n",
            "Counterfactual: [N] Bitcoin fails again to break resistance at $70K [C] BTC\n",
            "  ‚Üí Gate: 0 | Dir: SHORT | TP: 0.61%\n",
            "  ‚úì PASS: Direction flipped correctly\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Original: [N] Ethereum outperforms Bitcoin in Q4 rally [C] ETH\n",
            "  ‚Üí Gate: 0 | Dir: SHORT | TP: 0.82%\n",
            "Counterfactual: [N] Bitcoin outperforms Ethereum in Q4 rally [C] BTC\n",
            "  ‚Üí Gate: 0 | Dir: SHORT | TP: 0.82%\n",
            "  ‚úó FAIL: Direction didn't flip\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Original: [N] Experts say crypto winter is 'definitely' over this time [C] BTC\n",
            "  ‚Üí Gate: 0 | Dir: LONG | TP: 0.78%\n",
            "Counterfactual: [N] Experts confirm crypto winter continues with no end in sight [C] BTC\n",
            "  ‚Üí Gate: 0 | Dir: SHORT | TP: 0.56%\n",
            "  ‚úì PASS: Direction flipped correctly\n",
            "\n",
            "==================================================\n",
            "Counterfactual Test Result: 4/7 passed (57.1%)\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "OUT-OF-DISTRIBUTION EVALUATION\n",
            "==================================================\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but got index is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA__index_select)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2486548673.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mcausality_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounterfactual_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# ===== OOD EVALUATION =====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mood_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_ood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mood_test_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;31m# ===== FINAL REPORT =====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2486548673.py\u001b[0m in \u001b[0;36mevaluate_ood\u001b[0;34m(model, ood_df, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgate_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnexus_combined_loss_v4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgate_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1844668064.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, gate_labels, return_attentions)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# Get token-level embeddings (NO pooling!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    777\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, mask, inputs_embeds)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2540\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2542\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but got index is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA__index_select)"
          ]
        }
      ],
      "source": [
        "def evaluate_ood(model, ood_df, device):\n",
        "    \"\"\"OOD test set evaluation\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"OUT-OF-DISTRIBUTION EVALUATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    model.eval()\n",
        "    ood_dataset = NexusDatasetV2(ood_df)\n",
        "    ood_loader = DataLoader(ood_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    all_gate_preds = []\n",
        "    all_gate_gts = []\n",
        "    all_dir_preds = []\n",
        "    all_dir_gts = []\n",
        "    all_tp_preds = []\n",
        "    all_tp_gts = []\n",
        "    all_val_preds = []\n",
        "    all_val_gts = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, gate_gt, dir_gt, tp_gt, val_gt in ood_loader:\n",
        "            gate_gt = gate_gt.view(-1, 1).to(device)\n",
        "            dir_gt = dir_gt.view(-1, 1).to(device)\n",
        "            tp_gt = tp_gt.view(-1, 1).to(device)\n",
        "            val_gt = val_gt.view(-1, 1).to(device)\n",
        "\n",
        "            inputs = model.tokenizer(list(texts), return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
        "            preds = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], gate_labels=gate_gt)\n",
        "            loss, _ = nexus_combined_loss(preds, (gate_gt, dir_gt, tp_gt, val_gt))\n",
        "            val_loss += loss.item()\n",
        "            gate_p = preds['gate']\n",
        "            dir_p = preds['direction']\n",
        "            tp_p = preds['tp']\n",
        "            val_p = preds['validity']\n",
        "\n",
        "            gate_pred = (torch.sigmoid(gate_p) > 0.5).float()\n",
        "            all_gate_preds.extend(gate_pred.cpu().numpy())\n",
        "            all_gate_gts.extend(gate_gt.cpu().numpy())\n",
        "\n",
        "            action_mask = (gate_gt == 1).squeeze()\n",
        "            valid_dir_mask = (dir_gt >= 0).squeeze()  # -1 olan HOLD'larƒ± filtrele\n",
        "            combined_mask = action_mask & valid_dir_mask\n",
        "            if combined_mask.sum() > 0:\n",
        "                dir_pred = (torch.sigmoid(dir_p[combined_mask]) > 0.5).float()\n",
        "                all_dir_preds.extend(dir_pred.cpu().numpy())\n",
        "                all_dir_gts.extend(dir_gt[combined_mask].cpu().numpy())\n",
        "\n",
        "                all_tp_preds.extend(tp_p[action_mask].cpu().numpy())\n",
        "                all_tp_gts.extend(tp_gt[action_mask].cpu().numpy())\n",
        "\n",
        "                all_val_preds.extend(val_p[action_mask].cpu().numpy())\n",
        "                all_val_gts.extend(val_gt[action_mask].cpu().numpy())\n",
        "\n",
        "    # Metrics\n",
        "    gate_acc = accuracy_score(all_gate_gts, all_gate_preds)\n",
        "    gate_prec, gate_rec, gate_f1, _ = precision_recall_fscore_support(\n",
        "        all_gate_gts, all_gate_preds, average='binary', zero_division=0\n",
        "    )\n",
        "\n",
        "    print(f\"\\nGate Performance:\")\n",
        "    print(f\"  Accuracy: {gate_acc:.3f}\")\n",
        "    print(f\"  Precision: {gate_prec:.3f}\")\n",
        "    print(f\"  Recall: {gate_rec:.3f}\")\n",
        "    print(f\"  F1: {gate_f1:.3f}\")\n",
        "\n",
        "    dir_acc = 0.0\n",
        "    tp_mae = 0.0\n",
        "    val_mae = 0.0\n",
        "\n",
        "    if len(all_dir_preds) > 0:\n",
        "        dir_acc = accuracy_score(all_dir_gts, all_dir_preds)\n",
        "        print(f\"\\nDirection Performance (on actions):\")\n",
        "        print(f\"  Accuracy: {dir_acc:.3f}\")\n",
        "\n",
        "        tp_mae = mean_absolute_error(all_tp_gts, all_tp_preds)\n",
        "        tp_rmse = np.sqrt(mean_squared_error(all_tp_gts, all_tp_preds))\n",
        "\n",
        "        val_mae = mean_absolute_error(all_val_gts, all_val_preds)\n",
        "        val_rmse = np.sqrt(mean_squared_error(all_val_gts, all_val_preds))\n",
        "\n",
        "        print(f\"\\nRegression Performance:\")\n",
        "        print(f\"  TP - MAE: {tp_mae:.3f}% | RMSE: {tp_rmse:.3f}%\")\n",
        "        print(f\"  Validity - MAE: {val_mae:.3f}min | RMSE: {val_rmse:.3f}min\")\n",
        "\n",
        "    return {\n",
        "        'gate_acc': gate_acc,\n",
        "        'gate_f1': gate_f1,\n",
        "        'gate_precision': gate_prec,  # GPT: HOLD precision i√ßin\n",
        "        'dir_acc': dir_acc,\n",
        "        'tp_mae': tp_mae,\n",
        "        'val_mae': val_mae\n",
        "    }\n",
        "\n",
        "# ===== LOAD BEST MODEL =====\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LOADING BEST MODEL\")\n",
        "print(\"=\"*50)\n",
        "model = NexusV2Production(\"/content/drive/MyDrive/Nexus-AI-Models/Sentence-Transformers/Model1/nexus-lora-backbone-final\")\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Nexus-AI-Models/Sentence-Transformers/Model1/nexus_best.pt\"))\n",
        "print(\"‚úì Best model loaded\")\n",
        "# ===== COUNTERFACTUAL TEST =====\n",
        "causality_score = counterfactual_test(model)\n",
        "# ===== OOD EVALUATION =====\n",
        "ood_metrics = evaluate_ood(model, ood_test_df, device)\n",
        "# ===== FINAL REPORT =====\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL TRAINING REPORT\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
        "# EKLEME - HOLD precision kontrol√º\n",
        "print(f\"\\n‚ö†Ô∏è  CRITICAL METRICS CHECK:\")\n",
        "print(f\"  OOD Gate F1: {ood_metrics['gate_f1']:.3f} (Target: ‚â•0.70)\")\n",
        "print(f\"  HOLD Precision: {ood_metrics['gate_precision']:.3f} (Target: ‚â•0.85)\")\n",
        "print(f\"  Causality Score: {causality_score*100:.1f}% (Target: ‚â•80%)\")\n",
        "if ood_metrics['gate_f1'] < 0.70:\n",
        "    print(\"\\n‚ùå WARNING: Gate F1 too low. More real data needed!\")\n",
        "if ood_metrics['gate_precision'] < 0.85:\n",
        "    print(\"‚ùå WARNING: HOLD Precision too low. Will overtrade in production!\")\n",
        "if causality_score < 0.80:\n",
        "    print(\"‚ùå WARNING: Causality test failed. Model not learning causal patterns!\")\n",
        "print(\"\\n‚ö†Ô∏è  ARCHITECTURE RISKS:\")\n",
        "if ood_metrics['gate_f1'] < 0.65:\n",
        "    print(\"  - Low OOD F1: Consider hybrid triplet strategy (reasoning + self-contrast)\")\n",
        "print(\"  - Current triplet uses reasoning as positive (LLM style bias risk)\")\n",
        "print(\"  - Monitor performance on Telegram/Twitter news without reasoning\")\n",
        "print(f\"\\nOOD Test Performance:\")\n",
        "print(f\"  Gate Accuracy: {ood_metrics['gate_acc']:.3f}\")\n",
        "print(f\"  Direction Accuracy: {ood_metrics['dir_acc']:.3f}\")\n",
        "print(f\"  TP MAE: {ood_metrics['tp_mae']:.3f}%\")\n",
        "print(f\"  Validity MAE: {ood_metrics['val_mae']:.3f}min\")\n",
        "# ===== SAVE FINAL MODEL =====\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SAVING FINAL MODEL\")\n",
        "print(\"=\"*50)\n",
        "# Full model save\n",
        "model.encoder.save(\"nexus-final-encoder\")\n",
        "torch.save({\n",
        "    'gate_head': model.gate_head.state_dict(),\n",
        "    'dir_head': model.dir_head.state_dict(),\n",
        "    'tp_head': model.tp_head.state_dict(),\n",
        "    'val_head': model.val_head.state_dict(),\n",
        "    'config': {\n",
        "        'hidden_size': model.encoder.get_sentence_embedding_dimension(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'causality_score': causality_score,\n",
        "        'ood_metrics': ood_metrics\n",
        "    }\n",
        "}, \"nexus-final-heads.pt\")\n",
        "print(\"‚úì Final model saved to 'nexus-final-encoder' and 'nexus-final-heads.pt'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujupw69WgbY0",
        "outputId": "076831b9-c38b-423e-abb1-712685a3a7ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uso6lgKgRXs",
        "outputId": "b8254a4b-4f11-46a9-c828-c6e667af6397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'nexus_best.pt' successfully moved to '/content/drive/MyDrive/Nexus-AI-Models/Sentence-Transformers/Model1'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil # Import shutil for cross-filesystem moves\n",
        "\n",
        "source_file = 'nexus_best.pt'\n",
        "destination_path = '/content/drive/MyDrive/Nexus-AI-Models/Sentence-Transformers/Model1'\n",
        "\n",
        "if os.path.exists(source_file):\n",
        "    os.makedirs(destination_path, exist_ok=True)\n",
        "    # Use shutil.move instead of os.rename for cross-device links\n",
        "    shutil.move(source_file, os.path.join(destination_path, source_file))\n",
        "    print(f\"'{source_file}' successfully moved to '{destination_path}'\")\n",
        "else:\n",
        "    print(f\"Error: '{source_file}' not found in the current directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4o6wAcBgTva",
        "outputId": "fcc7190e-5187-4284-a143-98cdc1a68741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'/content/nexus-lora-backbone-final' klas√∂r√º ba≈üarƒ±yla '/content/drive/MyDrive/Nexus-AI-Models/Sentence-Transformers/Model1' konumuna ta≈üƒ±ndƒ±.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Ta≈üƒ±mak istediƒüiniz klas√∂r√ºn yolu\n",
        "source_folder = '/content/nexus-lora-backbone-final'\n",
        "\n",
        "# Google Drive'ƒ±nƒ±zdaki hedef yol\n",
        "# √ñrn: '/content/drive/MyDrive/YeniKlas√∂r√ºm'\n",
        "destination_path = '/content/drive/MyDrive/Nexus-AI-Models/Sentence-Transformers/Model1'\n",
        "\n",
        "# Klas√∂r√ºn varlƒ±ƒüƒ±nƒ± kontrol et\n",
        "if os.path.exists(source_folder):\n",
        "    # Hedef yolu olu≈ütur (eƒüer yoksa)\n",
        "    os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "    # Klas√∂r√º ta≈üƒ±\n",
        "    # shutil.move, farklƒ± dosya sistemleri arasƒ±nda bile √ßalƒ±≈üƒ±r\n",
        "    shutil.move(source_folder, os.path.join(destination_path, os.path.basename(source_folder)))\n",
        "    print(f\"'{source_folder}' klas√∂r√º ba≈üarƒ±yla '{destination_path}' konumuna ta≈üƒ±ndƒ±.\")\n",
        "else:\n",
        "    print(f\"Hata: '{source_folder}' mevcut dizinde bulunamadƒ±.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ef741bd06784adc89f6426c7203a330": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "185be10dd2c94b019333ad40eaa6b637": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a184c76134a646f1aa7e137a99fd1f1d",
              "IPY_MODEL_7ddc911873ea4becb1e8fde0b10d537a",
              "IPY_MODEL_f40726cc93b84cdaad35af6f72271dbd"
            ],
            "layout": "IPY_MODEL_43d2e4feba614833a4a605b61b3c14f1"
          }
        },
        "19b64c46557b4518934763b023903798": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d5ddce4014b49dd826f2bba415df52b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43d2e4feba614833a4a605b61b3c14f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "7a89d1d6f96144d2b7ed0ca3d7255fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ddc911873ea4becb1e8fde0b10d537a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ef741bd06784adc89f6426c7203a330",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a89d1d6f96144d2b7ed0ca3d7255fdb",
            "value": 1
          }
        },
        "8c5e7747f6764410b4794738dfc3575b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a184c76134a646f1aa7e137a99fd1f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19b64c46557b4518934763b023903798",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8c5e7747f6764410b4794738dfc3575b",
            "value": "Computing‚Äáwidget‚Äáexamples:‚Äá‚Äá‚Äá0%"
          }
        },
        "e1853d4aa5a243179fb14ed6ff08e126": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f40726cc93b84cdaad35af6f72271dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1853d4aa5a243179fb14ed6ff08e126",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1d5ddce4014b49dd826f2bba415df52b",
            "value": "‚Äá0/1‚Äá[00:00&lt;?,‚Äá?example/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
